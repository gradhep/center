{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "import jax import jax.numpy as jnp import numpy as np import pyhf # use jax backend of pyhf pyhf.set_backend(pyhf.tensor.jax_backend()) import smooth import smooth.infer . nBg = 8000 nSig = 300 background = np.random.normal(40, 10, nBg) signal = np.random.normal(50, 5, nSig) def generate_data(): return signal, background . import matplotlib.pyplot as plt bins = np.linspace(0, 80, 40) plt.figure(dpi=120) ax = plt.gca() plt.hist([background, signal], bins=bins, stacked=True, label=[&quot;background&quot;, &quot;signal&quot;]) ax.set(ylabel=&#39;frequency&#39;,xlabel=&#39;x&#39;) plt.legend(); . def preprocess(data_generator): def counts(cut_param): s, b = data_generator() s_counts = smooth.cut(s,&#39;&gt;&#39;,cut_param).sum() b_counts = smooth.cut(b,&#39;&gt;&#39;,cut_param).sum() return jnp.array([s_counts]), jnp.array([b_counts]) return counts . def simple_histosys(data_with_cuts, uncert): &quot;&quot;&quot; Makes a histosys model with up/down yields of +- bkg*(uncert/2). &quot;&quot;&quot; def from_spec(yields): s, b = yields bup, bdown = b * (1 + (uncert / 2)), b * (1 - (uncert / 2)) spec = { &quot;channels&quot;: [ { &quot;name&quot;: &quot;smoothcut&quot;, &quot;samples&quot;: [ { &quot;name&quot;: &quot;signal&quot;, &quot;data&quot;: s, &quot;modifiers&quot;: [ {&quot;name&quot;: &quot;mu&quot;, &quot;type&quot;: &quot;normfactor&quot;, &quot;data&quot;: None} ], }, { &quot;name&quot;: &quot;bkg&quot;, &quot;data&quot;: b, &quot;modifiers&quot;: [ { &quot;name&quot;: &quot;artificial_histosys&quot;, &quot;type&quot;: &quot;histosys&quot;, &quot;data&quot;: {&quot;lo_data&quot;: bdown, &quot;hi_data&quot;: bup,}, } ], }, ], }, ], } return pyhf.Model(spec) def model_maker(cut_pars): s, b = data_with_cuts(cut_pars) # make statistical model with pyhf m = from_spec([s, b]) nompars = m.config.suggested_init() bonlypars = jnp.asarray([x for x in nompars]) bonlypars = jax.ops.index_update(bonlypars, m.config.poi_index, 0.0) return m, bonlypars return model_maker . data_with_cuts = preprocess(generate_data) model_maker = simple_histosys(data_with_cuts, uncert=0.05) loss = smooth.infer.expected_pvalue_upper_limit(model_maker, solver_kwargs=dict(pdf_transform=True)) # loss2 = infer.upper_lim(loss) jax.value_and_grad(loss)(1.,40.) . (DeviceArray(0.00668244, dtype=float64), DeviceArray(-0.0461769, dtype=float64)) . loss2(40.) . Traced&lt;ShapedArray(float64[]):JaxprTrace(level=0/0)&gt; . DeviceArray(0., dtype=float64) . l=[] mus = np.linspace(0,1,100) [l.append(loss2(40.)(mu)) for mu in mus] import matplotlib.pyplot as plt plt.figure(dpi=120) plt.plot(mus,l) . min(l) . DeviceArray(0.00097437, dtype=float64) . def scn(c): return loss(c,1.) cuts_smooth = np.linspace(20, 70, 100) significances_smooth = np.asarray([scn(cut) for cut in cuts_smooth]) . def significance(S, B): return jnp.sqrt(2*((S+B)*jnp.log(1+S/B)-S)) def objective(data_with_cuts): def significance(S, B): return jnp.sqrt(2*((S+B)*jnp.log(1+S/B)-S)) def get_significance_smooth(cut_pars): &#39;&#39;&#39; Our loss function! Takes in cut parameters, returns significance &#39;&#39;&#39; s, b = data_with_cuts(cut_pars) return significance(s[0], b[0]) return get_significance_smooth get = objective(preprocess(generate_data)) cuts = np.linspace(20, 70, 500) significances = [1 - pyhf.tensorlib.normal_cdf(get(cut)) for cut in cuts] . import matplotlib.pyplot as plt plt.figure(dpi=120) # plt.plot(cuts, significances, c=&quot;C2&quot;, label=&#39;scan of Asimov significance&#39;) # plt.axhline(0.05,color=&#39;r&#39;,linestyle=&#39;--&#39;, label=&#39;$p_{ mu}$=0.05&#39;, alpha=0.4) plt.plot(cuts_smooth, significances_smooth, label=&#39;scan of $p_{ mu}$&#39;) plt.xlabel(&quot;cut position c&quot;) plt.ylabel(&quot;$p_{ mu}$&quot;) plt.axvline(cuts_smooth[np.argmin(significances_smooth)],linestyle=&#39;:&#39;, label=&#39;optimal cut&#39;) print(&quot;the optimal cut is c =&quot;, cuts_smooth[np.argmin(significances_smooth)]) plt.legend() plt.savefig(&#39;scanvfgegergds.png&#39;, bbox_inches=&#39;tight&#39;) . the optimal cut is c = 47.27272727272727 . import matplotlib.pyplot as plt plt.figure(dpi=120) plt.xlim([40,60]) plt.ylim([0,0.05]) plt.plot(cuts, significances, c=&quot;C2&quot;, label=&#39;scan of Asimov significance&#39;) # plt.axhline(0.05,color=&#39;r&#39;,linestyle=&#39;--&#39;, label=&#39;$p_{ mu}$=0.05&#39;, alpha=0.4) plt.plot(cuts_smooth, significances_smooth, label=&#39;scan of $p_{ mu}$&#39;) plt.xlabel(&quot;cut position c&quot;) plt.ylabel(&quot;$p_{ mu}$&quot;) plt.axvline(cuts_smooth[np.argmin(significances_smooth)],linestyle=&#39;:&#39;, label=&#39;optimal cut&#39;) plt.legend(); . def blobs(NMC=500, sig_mean = [-1, 1], b1_mean=[2.5, 2], b_mean=[1, -1], b2_mean=[-2.5, -1.5]): def generate_blobs(): bkg_up = np.random.multivariate_normal(b1_mean, [[1, 0], [0, 1]], size=(NMC,)) bkg_down = np.random.multivariate_normal(b2_mean, [[1, 0], [0, 1]], size=(NMC,)) bkg_nom = np.random.multivariate_normal(b_mean, [[1, 0], [0, 1]], size=(NMC,)) sig = np.random.multivariate_normal(sig_mean, [[1, 0], [0, 1]], size=(NMC,)) return sig, bkg_nom, bkg_up, bkg_down return generate_blobs . def hists(data_generator, predict, bins, bandwidth, LUMI=10, sig_scale = 2, bkg_scale = 10): def hist_maker(nn): s, b_nom, b_up, b_down = data_generator() NMC = len(s) nn_s, nn_b_nom, nn_b_up, nn_b_down = ( predict(nn, s).ravel(), predict(nn, b_nom).ravel(), predict(nn, b_up).ravel(), predict(nn, b_down).ravel(), ) kde_counts = jax.numpy.asarray([ smooth.hist(nn_s, bins, bandwidth) * sig_scale / NMC * LUMI, smooth.hist(nn_b_nom, bins, bandwidth) * bkg_scale / NMC * LUMI, smooth.hist(nn_b_up, bins, bandwidth) * bkg_scale / NMC * LUMI, smooth.hist(nn_b_down, bins, bandwidth) * bkg_scale / NMC * LUMI, ]) return kde_counts return hist_maker . def nn_histosys(histogram_maker): def from_spec(yields): s, b, bup, bdown = yields spec = { &quot;channels&quot;: [ { &quot;name&quot;: &quot;nn&quot;, &quot;samples&quot;: [ { &quot;name&quot;: &quot;signal&quot;, &quot;data&quot;: s, &quot;modifiers&quot;: [ {&quot;name&quot;: &quot;mu&quot;, &quot;type&quot;: &quot;normfactor&quot;, &quot;data&quot;: None} ], }, { &quot;name&quot;: &quot;bkg&quot;, &quot;data&quot;: b, &quot;modifiers&quot;: [ { &quot;name&quot;: &quot;nn_histosys&quot;, &quot;type&quot;: &quot;histosys&quot;, &quot;data&quot;: { &quot;lo_data&quot;: bdown, &quot;hi_data&quot;: bup, }, } ], }, ], }, ], } return pyhf.Model(spec) def nn_model_maker(nn): yields = histogram_maker(nn) m = from_spec(yields) nompars = m.config.suggested_init() bonlypars = jax.numpy.asarray([x for x in nompars]) bonlypars = jax.ops.index_update(bonlypars, m.config.poi_index, 0.0) return m, bonlypars return nn_model_maker . import jax.experimental.stax as stax # regression net init_random_params, predict = stax.serial( stax.Dense(1024), stax.Relu, stax.Dense(1024), stax.Relu, stax.Dense(1), stax.Sigmoid ) # choose hyperparams bins = np.linspace(0,1,4) centers = bins[:-1] + np.diff(bins)/2. bandwidth = 0.8 * 1/(len(bins)-1) # compose functions to define workflow data = blobs() hmaker = hists(data,predict,bins=bins,bandwidth=bandwidth) model = nn_histosys(hmaker) loss = smooth.infer.expected_pvalue_upper_limit(model, solver_kwargs=dict(pdf_transform=True)) _, network = init_random_params(jax.random.PRNGKey(13), (-1, 2)) jax.value_and_grad(loss, argnums=1)(1.0, network) . (DeviceArray(0.03419097, dtype=float64), [(DeviceArray([[-7.4747222e-05, -5.6944977e-05, 1.7446939e-05, ..., -2.4487432e-05, 6.9174617e-05, -3.5466583e-05], [ 1.2072978e-04, 2.1503494e-05, -6.7116935e-06, ..., 7.1674481e-06, -9.7872784e-05, -1.3499416e-06]], dtype=float32), DeviceArray([-4.6593788e-05, -2.4979156e-05, -1.8001270e-05, ..., 2.8620339e-05, 4.2742522e-05, -1.7730363e-05], dtype=float32)), (), (DeviceArray([[-7.4381433e-07, 8.7469658e-08, 6.7387840e-10, ..., -7.2362553e-08, 1.6154515e-06, 7.4637381e-07], [ 1.1795539e-07, 1.8024228e-07, 2.0555500e-09, ..., 0.0000000e+00, 1.3165605e-07, -6.8112592e-11], [-3.9922452e-07, 1.8413182e-10, 1.4307927e-10, ..., 1.6287530e-08, -5.0056929e-07, 4.5510586e-07], ..., [-3.5405691e-07, -1.1326374e-06, -1.4052329e-08, ..., 8.9071946e-08, -1.8953529e-06, 1.2026198e-06], [ 8.3317434e-07, 2.1319472e-06, 1.9097971e-08, ..., -2.9123530e-09, 3.0891329e-06, 1.2002033e-07], [ 2.0260664e-07, -3.5106780e-08, -3.7393555e-10, ..., 0.0000000e+00, 8.3604625e-08, -9.1771426e-09]], dtype=float32), DeviceArray([ 8.1915132e-06, 4.7153902e-05, 3.5778328e-07, ..., -6.6411008e-07, 8.6970671e-05, 1.8307695e-05], dtype=float32)), (), (DeviceArray([[-2.1727724e-06], [-2.5328352e-05], [-1.3518069e-05], ..., [-8.2271417e-06], [-2.6815096e-04], [ 1.0678576e-05]], dtype=float32), DeviceArray([-0.00260607], dtype=float32)), ()]) . import jax.experimental.optimizers as optimizers import time opt_init, opt_update, opt_params = optimizers.adam(1e-3) @jax.jit def update_and_value(i, opt_state, mu): net = opt_params(opt_state) value, grad = jax.value_and_grad(loss,argnums=1)(mu, net) return opt_update(i, grad, state), value, net def train_network(N): cls_vals = [] _, network = init_random_params(jax.random.PRNGKey(1), (-1, 2)) state = opt_init(network) losses = [] # parameter update function #@jax.jit def update_and_value(i, opt_state, mu): net = opt_params(opt_state) value, grad = jax.value_and_grad(loss,argnums=1)(mu, net) return opt_update(i, grad, state), value, net for i in range(N): start_time = time.time() state, value, network = update_and_value(i,state,1.0) epoch_time = time.time() - start_time losses.append(value) metrics = {&quot;loss&quot;: losses} yield network, metrics, epoch_time . maxN = 50 # make me bigger for better results! # Training for i, (network, metrics, epoch_time) in enumerate(train_network(maxN)): print(f&quot;epoch {i}:&quot;, f&#39;p_mu = {metrics[&quot;loss&quot;][-1]:.5f}, took {epoch_time:.2f}s&#39;) . epoch 0: p_mu = 0.03416, took 1.71s epoch 1: p_mu = 0.01664, took 2.05s epoch 2: p_mu = 0.00774, took 2.03s epoch 3: p_mu = 0.00423, took 2.05s epoch 4: p_mu = 0.00264, took 2.05s epoch 5: p_mu = 0.00185, took 2.06s epoch 6: p_mu = 0.00142, took 2.11s epoch 7: p_mu = 0.00116, took 2.02s epoch 8: p_mu = 0.00100, took 2.01s epoch 9: p_mu = 0.00089, took 2.06s epoch 10: p_mu = 0.00081, took 2.02s epoch 11: p_mu = 0.00076, took 2.12s epoch 12: p_mu = 0.00072, took 2.13s epoch 13: p_mu = 0.00069, took 2.13s epoch 14: p_mu = 0.00068, took 2.05s epoch 15: p_mu = 0.00067, took 2.07s epoch 16: p_mu = 0.00066, took 2.03s epoch 17: p_mu = 0.00064, took 2.06s epoch 18: p_mu = 0.00063, took 2.06s epoch 19: p_mu = 0.00062, took 2.05s epoch 20: p_mu = 0.00060, took 2.06s . KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-13-265cf9dd44a3&gt; in &lt;module&gt; 2 3 # Training -&gt; 4 for i, (network, metrics, epoch_time) in enumerate(train_network(maxN)): 5 print(f&#34;epoch {i}:&#34;, f&#39;p_mu = {metrics[&#34;loss&#34;][-1]:.5f}, took {epoch_time:.2f}s&#39;) &lt;ipython-input-12-913bebde21bd&gt; in train_network(N) 26 for i in range(N): 27 start_time = time.time() &gt; 28 state, value, network = update_and_value(i,state,1.0) 29 epoch_time = time.time() - start_time 30 losses.append(value) &lt;ipython-input-12-913bebde21bd&gt; in update_and_value(i, opt_state, mu) 22 net = opt_params(opt_state) 23 value, grad = jax.value_and_grad(loss,argnums=1)(mu, net) &gt; 24 return opt_update(i, grad, state), value, net 25 26 for i in range(N): ~/envs/neos/lib/python3.7/site-packages/jax/experimental/optimizers.py in tree_update(i, grad_tree, opt_state) 152 raise TypeError(msg.format(tree, tree2)) 153 states = map(tree_unflatten, subtrees, packed_state) --&gt; 154 new_states = map(partial(update, i), grad_flat, states) 155 new_states_flat, subtrees2 = unzip2(map(tree_flatten, new_states)) 156 for subtree, subtree2 in zip(subtrees, subtrees2): ~/envs/neos/lib/python3.7/site-packages/jax/util.py in safe_map(f, *args) 32 for arg in args[1:]: 33 assert len(arg) == n, &#39;length mismatch: {}&#39;.format(list(map(len, args))) &gt; 34 return list(map(f, *args)) 35 36 def unzip2(xys): ~/envs/neos/lib/python3.7/site-packages/jax/experimental/optimizers.py in update(i, g, state) 373 x, m, v = state 374 m = (1 - b1) * g + b1 * m # First moment estimate. --&gt; 375 v = (1 - b2) * (g ** 2) + b2 * v # Second moment estimate. 376 mhat = m / (1 - b1 ** (i + 1)) # Bias correction. 377 vhat = v / (1 - b2 ** (i + 1)) ~/envs/neos/lib/python3.7/site-packages/jax/numpy/lax_numpy.py in deferring_binary_op(self, other) 3756 if not isinstance(other, _scalar_types + _arraylike_types + (core.Tracer,)): 3757 return NotImplemented -&gt; 3758 return binary_op(self, other) 3759 return deferring_binary_op 3760 ~/envs/neos/lib/python3.7/site-packages/jax/numpy/lax_numpy.py in power(x1, x2) 603 dtype = _dtype(x1) 604 if not issubdtype(dtype, integer): --&gt; 605 return lax.pow(x1, x2) 606 607 # Integer power =&gt; use binary exponentiation. ~/envs/neos/lib/python3.7/site-packages/jax/lax/lax.py in pow(x, y) 249 def pow(x: Array, y: Array) -&gt; Array: 250 r&#34;&#34;&#34;Elementwise power: :math:`x^y`.&#34;&#34;&#34; --&gt; 251 return pow_p.bind(x, y) 252 253 def sqrt(x: Array) -&gt; Array: ~/envs/neos/lib/python3.7/site-packages/jax/core.py in bind(self, *args, **kwargs) 197 top_trace = find_top_trace(args) 198 if top_trace is None: --&gt; 199 return self.impl(*args, **kwargs) 200 201 tracers = map(top_trace.full_raise, args) ~/envs/neos/lib/python3.7/site-packages/jax/interpreters/xla.py in apply_primitive(prim, *args, **params) 165 &#34;&#34;&#34;Impl rule that compiles and runs a single primitive &#39;prim&#39; using XLA.&#34;&#34;&#34; 166 compiled_fun = xla_primitive_callable(prim, *map(arg_spec, args), **params) --&gt; 167 return compiled_fun(*args) 168 169 @cache() ~/envs/neos/lib/python3.7/site-packages/jax/interpreters/xla.py in _execute_compiled_primitive(prim, compiled, result_handler, *args) 255 device, = compiled.local_devices() 256 input_bufs = [device_put(x, device) for x in args if x is not token] --&gt; 257 out_bufs = compiled.Execute(input_bufs) 258 if FLAGS.jax_debug_nans: 259 check_nans(prim, out_bufs) KeyboardInterrupt: .",
            "url": "https://gradhep.github.io/center/jupyter/2020/05/11/sup.html",
            "relUrl": "/jupyter/2020/05/11/sup.html",
            "date": " • May 11, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://gradhep.github.io/center/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://gradhep.github.io/center/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://gradhep.github.io/center/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gradhep.github.io/center/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}